{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "Talk is cheat , Show me the code ! \r\n",
                "根据代码实现了线性回归的操作过程，也知道了adagrad的大致原理\r\n",
                "\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import numpy as np\r\n",
                "import matplotlib.pyplot as plt\r\n",
                "from pylab import mpl\r\n",
                "\r\n",
                "# matplotlib没有中文字体，动态解决\r\n",
                "plt.rcParams['font.sans-serif'] = ['Simhei']  # 显示中文\r\n",
                "mpl.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "x_d = np.asarray( [338., 333., 328., 207., 226., 25., 179., 60., 208., 606.]   )\r\n",
                "y_d = np.asarray( [640., 633., 619., 393., 428., 27., 193., 66., 226., 1591.]  )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 求拟合曲线的值"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# 定义损失函数\r\n",
                "def loss_function(y_d,y_hat,m):\r\n",
                "    return np.dot(y_d-y_hat, y_d-y_hat)/m/2\r\n",
                "\r\n",
                "# 定义 b 梯度函数\r\n",
                "def grad_b_function(y_d, y_hat, m):\r\n",
                "    return -1 * np.sum(y_d - y_hat) / m\r\n",
                "\r\n",
                "# 定义 w 梯度函数\r\n",
                "def grad_w_function(y_d, y_hat, m):\r\n",
                "    return -1 * np.dot(y_d - y_hat, x_d) / m\r\n",
                "\r\n",
                "# linear regression\r\n",
                "#b = -120\r\n",
                "#w = -4\r\n",
                "m = float(len(x_d))\r\n",
                "b=-2\r\n",
                "w=0.01\r\n",
                "lr = 0.000005\r\n",
                "iteration = 1400000\r\n",
                "\r\n",
                "b_history = [b]\r\n",
                "w_history = [w]\r\n",
                "loss_history = []\r\n",
                "import time\r\n",
                "start = time.time()\r\n",
                "for i in range(iteration):\r\n",
                "    y_hat = w * x_d  +b\r\n",
                "    loss = loss_function(y_d , y_hat , m )\r\n",
                "    grad_b = grad_b_function(y_d, y_hat, m)\r\n",
                "    grad_w = grad_w_function(y_d, y_hat, m)\r\n",
                "    # update param\r\n",
                "    b -= lr * grad_b\r\n",
                "    w -= lr * grad_w\r\n",
                "\r\n",
                "    b_history.append(b)\r\n",
                "    w_history.append(w)\r\n",
                "    loss_history.append(loss)\r\n",
                "    if i % 50000 == 0:\r\n",
                "        print(\"Step %7d, w: %0.4f, b: %4.4f, Loss: %.4f\" % (i, w, b, loss))\r\n",
                "end = time.time()\r\n",
                "print(\"大约需要时间：\",end-start)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Step       0, w: 0.9374, b: -1.9976, Loss: 206894.6910\n",
                        "Step   50000, w: 2.1697, b: -14.5410, Loss: 9310.2246\n",
                        "Step  100000, w: 2.2034, b: -26.2434, Loss: 8762.2094\n",
                        "Step  150000, w: 2.2347, b: -37.1583, Loss: 8285.4718\n",
                        "Step  200000, w: 2.2640, b: -47.3386, Loss: 7870.7413\n",
                        "Step  250000, w: 2.2913, b: -56.8339, Loss: 7509.9528\n",
                        "Step  300000, w: 2.3167, b: -65.6901, Loss: 7196.0903\n",
                        "Step  350000, w: 2.3405, b: -73.9504, Loss: 6923.0504\n",
                        "Step  400000, w: 2.3626, b: -81.6547, Loss: 6685.5235\n",
                        "Step  450000, w: 2.3832, b: -88.8406, Loss: 6478.8905\n",
                        "Step  500000, w: 2.4025, b: -95.5429, Loss: 6299.1334\n",
                        "Step  550000, w: 2.4205, b: -101.7941, Loss: 6142.7564\n",
                        "Step  600000, w: 2.4372, b: -107.6247, Loss: 6006.7186\n",
                        "Step  650000, w: 2.4529, b: -113.0629, Loss: 5888.3746\n",
                        "Step  700000, w: 2.4674, b: -118.1351, Loss: 5785.4230\n",
                        "Step  750000, w: 2.4810, b: -122.8659, Loss: 5695.8618\n",
                        "Step  800000, w: 2.4937, b: -127.2784, Loss: 5617.9495\n",
                        "Step  850000, w: 2.5055, b: -131.3940, Loss: 5550.1708\n",
                        "Step  900000, w: 2.5166, b: -135.2325, Loss: 5491.2078\n",
                        "Step  950000, w: 2.5269, b: -138.8128, Loss: 5439.9138\n",
                        "Step 1000000, w: 2.5365, b: -142.1521, Loss: 5395.2914\n",
                        "Step 1050000, w: 2.5454, b: -145.2667, Loss: 5356.4728\n",
                        "Step 1100000, w: 2.5538, b: -148.1716, Loss: 5322.7031\n",
                        "Step 1150000, w: 2.5615, b: -150.8811, Loss: 5293.3257\n",
                        "Step 1200000, w: 2.5688, b: -153.4083, Loss: 5267.7693\n",
                        "Step 1250000, w: 2.5756, b: -155.7653, Loss: 5245.5369\n",
                        "Step 1300000, w: 2.5819, b: -157.9638, Loss: 5226.1961\n",
                        "Step 1350000, w: 2.5878, b: -160.0143, Loss: 5209.3710\n",
                        "大约需要时间： 50.55351686477661\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# 函数，画出参数变化曲线\r\n",
                "def plot_history_of_w_b(m, b_history, w_history):\r\n",
                "    # plot the figure\r\n",
                "    x = np.arange(-200, -100, 1)\r\n",
                "    y = np.arange(-5, 5, 0.1)\r\n",
                "    Z = np.zeros((len(x), len(y)))\r\n",
                "    X, Y = np.meshgrid(x, y)\r\n",
                "\r\n",
                "\r\n",
                "    # loss\r\n",
                "    for i in range(len(x)):\r\n",
                "        for j in range(len(y)):\r\n",
                "            b = x[i]\r\n",
                "            w = y[j]\r\n",
                "            Z[j][i] = 0  # meshgrid吐出结果：y为行，x为列\r\n",
                "            for n in range(len(x_d)):\r\n",
                "                Z[j][i] += (y_d[n] - b - w * x_d[n]) ** 2\r\n",
                "            Z[j][i] /= len(x_d)\r\n",
                "\r\n",
                "    plt.contourf(x, y, Z, 50, alpha=0.5, cmap=plt.get_cmap('jet'))  # 填充等高线\r\n",
                "    plt.plot([-188.4], [2.67], 'x', ms=12, mew=3, color=\"orange\")\r\n",
                "    plt.plot(b_history, w_history, 'o-', ms=3, lw=1.5, color='black')\r\n",
                "    plt.xlim(-200, -100)\r\n",
                "    plt.ylim(-5, 5)\r\n",
                "    plt.xlabel(r'$b$')\r\n",
                "    plt.ylabel(r'$w$')\r\n",
                "    plt.title(\"线性回归\")\r\n",
                "    plt.show()\r\n",
                "plot_history_of_w_b(m,b_history,w_history)"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ],
                        "image/png": "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"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "# 定义损失函数\r\n",
                "def loss_function(y_d,y_hat,m):\r\n",
                "    return np.dot(y_d-y_hat, y_d-y_hat)/m/2\r\n",
                "\r\n",
                "# 定义 b 梯度函数\r\n",
                "def grad_b_function(y_d, y_hat, m):\r\n",
                "    return -1 * np.sum(y_d - y_hat) / m\r\n",
                "\r\n",
                "# 定义 w 梯度函数\r\n",
                "def grad_w_function(y_d, y_hat, m):\r\n",
                "    return -1 * np.dot(y_d - y_hat, x_d) / m\r\n",
                "\r\n",
                "# linear regression\r\n",
                "#b = -120\r\n",
                "#w = -4\r\n",
                "m = float(len(x_d))\r\n",
                "b=-2\r\n",
                "w=0.01\r\n",
                "lr = 0.000005\r\n",
                "iteration = 1400000\r\n",
                "\r\n",
                "b_history = [b]\r\n",
                "w_history = [w]\r\n",
                "loss_history = []\r\n",
                "import time\r\n",
                "start = time.time()\r\n",
                "for i in range(iteration):\r\n",
                "    y_hat = w * x_d  +b\r\n",
                "    loss = loss_function(y_d , y_hat , m )\r\n",
                "    grad_b = grad_b_function(y_d, y_hat, m)\r\n",
                "    grad_w = grad_w_function(y_d, y_hat, m)\r\n",
                "    # update param\r\n",
                "    # lr = lr**i/np.sqrt(i+1)\r\n",
                "\r\n",
                "    b -= lr * grad_b\r\n",
                "    w -= lr * grad_w\r\n",
                "\r\n",
                "    b_history.append(b)\r\n",
                "    w_history.append(w)\r\n",
                "    loss_history.append(loss)\r\n",
                "    if i % 50000 == 0:\r\n",
                "        print(\"Step %7d, w: %0.4f, b: %4.4f, Loss: %.4f\" % (i, w, b, loss))\r\n",
                "end = time.time()\r\n",
                "print(\"大约需要时间：\",end-start)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Step       0, w: 0.9374, b: -1.9976, Loss: 206894.6910\n",
                        "Step   50000, w: 2.1697, b: -14.5410, Loss: 9310.2246\n",
                        "Step  100000, w: 2.2034, b: -26.2434, Loss: 8762.2094\n",
                        "Step  150000, w: 2.2347, b: -37.1583, Loss: 8285.4718\n",
                        "Step  200000, w: 2.2640, b: -47.3386, Loss: 7870.7413\n",
                        "Step  250000, w: 2.2913, b: -56.8339, Loss: 7509.9528\n",
                        "Step  300000, w: 2.3167, b: -65.6901, Loss: 7196.0903\n",
                        "Step  350000, w: 2.3405, b: -73.9504, Loss: 6923.0504\n",
                        "Step  400000, w: 2.3626, b: -81.6547, Loss: 6685.5235\n",
                        "Step  450000, w: 2.3832, b: -88.8406, Loss: 6478.8905\n",
                        "Step  500000, w: 2.4025, b: -95.5429, Loss: 6299.1334\n",
                        "Step  550000, w: 2.4205, b: -101.7941, Loss: 6142.7564\n",
                        "Step  600000, w: 2.4372, b: -107.6247, Loss: 6006.7186\n",
                        "Step  650000, w: 2.4529, b: -113.0629, Loss: 5888.3746\n",
                        "Step  700000, w: 2.4674, b: -118.1351, Loss: 5785.4230\n",
                        "Step  750000, w: 2.4810, b: -122.8659, Loss: 5695.8618\n",
                        "Step  800000, w: 2.4937, b: -127.2784, Loss: 5617.9495\n",
                        "Step  850000, w: 2.5055, b: -131.3940, Loss: 5550.1708\n",
                        "Step  900000, w: 2.5166, b: -135.2325, Loss: 5491.2078\n",
                        "Step  950000, w: 2.5269, b: -138.8128, Loss: 5439.9138\n",
                        "Step 1000000, w: 2.5365, b: -142.1521, Loss: 5395.2914\n",
                        "Step 1050000, w: 2.5454, b: -145.2667, Loss: 5356.4728\n",
                        "Step 1100000, w: 2.5538, b: -148.1716, Loss: 5322.7031\n",
                        "Step 1150000, w: 2.5615, b: -150.8811, Loss: 5293.3257\n",
                        "Step 1200000, w: 2.5688, b: -153.4083, Loss: 5267.7693\n",
                        "Step 1250000, w: 2.5756, b: -155.7653, Loss: 5245.5369\n",
                        "Step 1300000, w: 2.5819, b: -157.9638, Loss: 5226.1961\n",
                        "Step 1350000, w: 2.5878, b: -160.0143, Loss: 5209.3710\n",
                        "大约需要时间： 47.4529972076416\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}