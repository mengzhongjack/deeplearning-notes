# 网络设计技巧


这个阶段的技巧有：· 固定随机种子使用固定的随机种子，来保证运行代码两次都获得相同的结果，消除差异因素。· 简单化在此阶段不要有任何幻想，不要扩增数据。扩增数据后面会用到，但是在这里不要使用，现在引入只会导致错误。· 在评估中添加有效数字在绘制测试集损失时，对整个测试集进行评估，不要只绘制批次测试损失图像，然后用Tensorboard对它们进行平滑处理。· 在初始阶段验证损失函数验证函数是否从正确的损失值开始。例如，如果正确初始化最后一层，则应在softmax初始化时测量-log(1/n_classes)。· 初始化正确初始化最后一层的权重。如果回归一些平均值为50的值，则将最终偏差初始化为50。如果有一个比例为1:10的不平衡数据集，请设置对数的偏差，使网络预测概率在初始化时为0.1。正确设置这些可以加速模型的收敛。· 人类基线监控除人为可解释和可检查的损失之外的指标。尽可能评估人的准确性并与之进行比较。或者对测试数据进行两次注释，并且对于每个示例，将一个注释视为预测，将第二个注释视为事实。· 设置一个独立于输入的基线最简单的方法是将所有输入设置为零，看看模型是否学会从输入中提取任何信息。· 过拟合一个batch增加了模型的容量并验证我们可以达到的最低损失。· 验证减少训练损失尝试稍微增加数据容量。· 在训练模型前进行数据可视化将原始张量的数据和标签可视化，可以节省了调试次数，并揭示了数据预处理和数据扩增中的问题。· 可视化预测动态在训练过程中对固定测试批次上的模型预测进行可视化。· 使用反向传播来获得依赖关系：一个方法是将第i个样本的损失设置为1.0，运行反向传播一直到输入，并确保仅在第i个样本上有非零的梯度。· 概括一个特例：对正在做的事情编写一个非常具体的函数，让它运行，然后在以后过程中确保能得到相同的结果。3、过拟合首先我们得有一个足够大的模型，它可以过拟合，减少训练集上的损失，然后适当地调整它，放弃一些训练集损失，改善在验证集上的损失）。这一阶段的技巧有：· 挑选模型为了获得较好的训练损失，我们需要为数据选择合适的架构。不要总想着一步到位。如果要做图像分类，只需复制粘贴ResNet-50，我们可以在稍后的过程中做一些自定义的事。· Adam方法是安全的在设定基线的早期阶段，使用学习率为3e-4的Adam 。根据经验，亚当对超参数更加宽容，包括不良的学习率。· 一次只复杂化一个如果多个信号输入分类器，建议逐个输入，然后增加复杂性，确保预期的性能逐步提升，而不要一股脑儿全放进去。比如，尝试先插入较小的图像，然后再将它们放大。· 不要相信学习率衰减默认值如果不小心，代码可能会过早地将学习率减少到零，导致模型无法收敛。我们完全禁用学习率衰减避免这种状况的发生。4、正则化理想的话，我们现在有一个大模型，在训练集上拟合好了。现在，该正则化了。舍弃一点训练集上的准确率，可以换取验证集上的准确率。这里有一些技巧：· 获取更多数据至今大家最偏爱的正则化方法，就是添加一些真实训练数据。不要在一个小数据集花太大功夫，试图搞出大事情来。有精力去多收集点数据，这是唯一一个确保性能单调提升的方法。· 数据扩增把数据集做大，除了继续收集数据之外，就是扩增了。旋转，翻转，拉伸，做扩增的时候可以野性一点。· 有创意的扩增还有什么办法扩增数据集？比如域随机化 (Domain Randomization) ，模拟 (Simulation) ，巧妙的混合 (Hybrids) ，比如把数据插进场景里去。甚至可以用上GAN。· 预训练当然，就算你手握充足的数据，直接用预训练模型也没坏处。· 跟监督学习死磕不要对无监督预训练太过兴奋了。至少在视觉领域，无监督到现在也没有非常强大的成果。虽然，NLP领域有了BERT，有了会讲故事的GPT-2，但我们看到的效果很大程度上还是经过了人工挑选。· 输入低维一点把那些可能包含虚假信号的特征去掉，因为这些东西很可能造成过拟合，尤其是数据集不大的时候。同理，如果低层细节不是那么重要的话，就输入小一点的图片，捕捉高层信息就好了。· 模型小一点许多情况下，都可以给网络加上领域知识限制 (Domain Knowledge Constraints) ，来把模型变小。比如，以前很流行在ImageNet的骨架上放全连接层，但现在这种操作已经被平均池化取代了，大大减少了参数。·减小批尺寸对批量归一化 (Batch Normalization) 这项操作来说，小批量可能带来更好的正则化效果 (Regularization) 。· Dropout给卷积网络用dropout2d。不过使用需谨慎，因为这种操作似乎跟批量归一化不太合得来。· 权重衰减增加权重衰减 (Weight Decay) 的惩罚力度。· 早停法不用一直一直训练，可以观察验证集的损失，在快要过拟合的时候，及时喊停。· 也试试大点的模型注意，这条紧接上条 (且仅接上条) 。我发现，大模型很容易过拟合，几乎是必然，但早停的话，模型可以表现很好。最后的最后，如果想要更加确信，自己训练出的网络，是个不错的分类器，就把第一层的权重可视化一下，看看边缘 (Edges) 美不美。如果第一层的过滤器看起来像噪音，就需要再搞一搞了。同理，激活 (Activations) 有时候也会看出瑕疵来，那样就要研究一下哪里出了问题。5、调参读到这里，你的AI应该已经开始探索广阔天地了。这里，有几件事需要注意。· 随机网格搜索在同时调整多个超参数的情况下，网格搜索听起来是很诱人，可以把各种设定都包含进来。但是要记住，随机搜索才是最好的。直觉上说，这是因为网络通常对其中一些参数比较敏感，对其他参数不那么敏感。如果参数a是有用的，参数b起不了什么作用，就应该对a取样更彻底一些，不要只在几个固定点上多次取样。· 超参数优化世界上，有许多许多靓丽的贝叶斯超参数优化工具箱，很多小伙伴也给了这些工具好评。但我个人的经验是，State-of-the-Art都是用实习生做出来的 (误) 。
