{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "import numpy as np\r\n",
                "import matplotlib.pyplot as plt\r\n",
                "from pylab import mpl\r\n",
                "\r\n",
                "# matplotlib没有中文字体，动态解决\r\n",
                "plt.rcParams['font.sans-serif'] = ['Simhei']  # 显示中文\r\n",
                "mpl.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "x_data = [338., 333., 328., 207., 226., 25., 179., 60., 208., 606.]\r\n",
                "y_data = [640., 633., 619., 393., 428., 27., 193., 66., 226., 1591.]\r\n",
                "x_d = np.asarray(x_data)\r\n",
                "y_d = np.asarray(y_data)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "x = np.arange(-200, -100, 1)\r\n",
                "y = np.arange(-5, 5, 0.1)\r\n",
                "Z = np.zeros((len(x), len(y)))\r\n",
                "X, Y = np.meshgrid(x, y)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "# loss\r\n",
                "# for i in range(len(x)):\r\n",
                "#     for j in range(len(y)):\r\n",
                "#         b = x[i]\r\n",
                "#         w = y[j]\r\n",
                "#         Z[j][i] = 0  # meshgrid吐出结果：y为行，x为列\r\n",
                "#         for n in range(len(x_data)):\r\n",
                "#             Z[j][i] += (y_data[n] - b - w * x_data[n]) ** 2\r\n",
                "#         Z[j][i] /= len(x_data)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# 定义损失函数\r\n",
                "def loss_function(y_d,y_hat,m):\r\n",
                "    return np.dot(y_d-y_hat, y_d-y_hat)/m\r\n",
                "\r\n",
                "# 定义"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "# linear regression\r\n",
                "#b = -120\r\n",
                "#w = -4\r\n",
                "b=-2\r\n",
                "w=0.01\r\n",
                "lr = 0.000005\r\n",
                "iteration = 1400000\r\n",
                "\r\n",
                "b_history = [b]\r\n",
                "w_history = [w]\r\n",
                "loss_history = []\r\n",
                "import time\r\n",
                "start = time.time()\r\n",
                "for i in range(iteration):\r\n",
                "    m = float(len(x_d))\r\n",
                "    y_hat = w * x_d  +b\r\n",
                "    loss = loss_function(y_d , y_hat , m )\r\n",
                "    grad_b = -2.0 * np.sum(y_d - y_hat) / m\r\n",
                "    grad_w = -2.0 * np.dot(y_d - y_hat, x_d) / m\r\n",
                "    # update param\r\n",
                "    b -= lr * grad_b\r\n",
                "    w -= lr * grad_w\r\n",
                "\r\n",
                "    b_history.append(b)\r\n",
                "    w_history.append(w)\r\n",
                "    loss_history.append(loss)\r\n",
                "    if i % 10000 == 0:\r\n",
                "        print(\"Step %i, w: %0.4f, b: %.4f, Loss: %.4f\" % (i, w, b, loss))\r\n",
                "end = time.time()\r\n",
                "print(\"大约需要时间：\",end-start)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Step 0, w: 1.8648, b: -1.9952, Loss: 413789.3821\n",
                        "Step 10000, w: 2.1484, b: -7.1183, Loss: 19355.2329\n",
                        "Step 20000, w: 2.1627, b: -12.1013, Loss: 18858.5836\n",
                        "Step 30000, w: 2.1766, b: -16.9474, Loss: 18388.8578\n",
                        "Step 40000, w: 2.1902, b: -21.6603, Loss: 17944.5958\n",
                        "Step 50000, w: 2.2034, b: -26.2436, Loss: 17524.4173\n",
                        "Step 60000, w: 2.2162, b: -30.7010, Loss: 17127.0167\n",
                        "Step 70000, w: 2.2286, b: -35.0359, Loss: 16751.1593\n",
                        "Step 80000, w: 2.2407, b: -39.2517, Loss: 16395.6772\n",
                        "Step 90000, w: 2.2525, b: -43.3516, Loss: 16059.4658\n",
                        "Step 100000, w: 2.2640, b: -47.3389, Loss: 15741.4804\n",
                        "Step 110000, w: 2.2751, b: -51.2165, Loss: 15440.7331\n",
                        "Step 120000, w: 2.2860, b: -54.9876, Loss: 15156.2893\n",
                        "Step 130000, w: 2.2965, b: -58.6551, Loss: 14887.2653\n",
                        "Step 140000, w: 2.3067, b: -62.2217, Loss: 14632.8251\n",
                        "Step 150000, w: 2.3167, b: -65.6903, Loss: 14392.1781\n",
                        "Step 160000, w: 2.3264, b: -69.0637, Loss: 14164.5766\n",
                        "Step 170000, w: 2.3358, b: -72.3442, Loss: 13949.3134\n",
                        "Step 180000, w: 2.3450, b: -75.5347, Loss: 13745.7197\n",
                        "Step 190000, w: 2.3539, b: -78.6374, Loss: 13553.1628\n",
                        "Step 200000, w: 2.3626, b: -81.6549, Loss: 13371.0444\n",
                        "Step 210000, w: 2.3710, b: -84.5895, Loss: 13198.7988\n",
                        "Step 220000, w: 2.3792, b: -87.4434, Loss: 13035.8905\n",
                        "Step 230000, w: 2.3872, b: -90.2189, Loss: 12881.8136\n",
                        "Step 240000, w: 2.3950, b: -92.9181, Loss: 12736.0891\n",
                        "Step 250000, w: 2.4025, b: -95.5431, Loss: 12598.2644\n",
                        "Step 260000, w: 2.4098, b: -98.0960, Loss: 12467.9112\n",
                        "Step 270000, w: 2.4170, b: -100.5787, Loss: 12344.6245\n",
                        "Step 280000, w: 2.4239, b: -102.9931, Loss: 12228.0211\n",
                        "Step 290000, w: 2.4307, b: -105.3413, Loss: 12117.7389\n",
                        "Step 300000, w: 2.4372, b: -107.6249, Loss: 12013.4350\n",
                        "Step 310000, w: 2.4436, b: -109.8457, Loss: 11914.7855\n",
                        "Step 320000, w: 2.4498, b: -112.0055, Loss: 11821.4838\n",
                        "Step 330000, w: 2.4559, b: -114.1059, Loss: 11733.2400\n",
                        "Step 340000, w: 2.4617, b: -116.1486, Loss: 11649.7798\n",
                        "Step 350000, w: 2.4674, b: -118.1352, Loss: 11570.8441\n",
                        "Step 360000, w: 2.4730, b: -120.0672, Loss: 11496.1875\n",
                        "Step 370000, w: 2.4784, b: -121.9461, Loss: 11425.5780\n",
                        "Step 380000, w: 2.4836, b: -123.7733, Loss: 11358.7963\n",
                        "Step 390000, w: 2.4887, b: -125.5504, Loss: 11295.6348\n",
                        "Step 400000, w: 2.4937, b: -127.2785, Loss: 11235.8973\n",
                        "Step 410000, w: 2.4985, b: -128.9592, Loss: 11179.3982\n",
                        "Step 420000, w: 2.5032, b: -130.5937, Loss: 11125.9619\n",
                        "Step 430000, w: 2.5078, b: -132.1833, Loss: 11075.4224\n",
                        "Step 440000, w: 2.5122, b: -133.7292, Loss: 11027.6227\n",
                        "Step 450000, w: 2.5166, b: -135.2326, Loss: 10982.4142\n",
                        "Step 460000, w: 2.5208, b: -136.6947, Loss: 10939.6564\n",
                        "Step 470000, w: 2.5249, b: -138.1167, Loss: 10899.2166\n",
                        "Step 480000, w: 2.5288, b: -139.4995, Loss: 10860.9690\n",
                        "Step 490000, w: 2.5327, b: -140.8443, Loss: 10824.7948\n",
                        "Step 500000, w: 2.5365, b: -142.1522, Loss: 10790.5816\n",
                        "Step 510000, w: 2.5401, b: -143.4241, Loss: 10758.2231\n",
                        "Step 520000, w: 2.5437, b: -144.6611, Loss: 10727.6188\n",
                        "Step 530000, w: 2.5471, b: -145.8641, Loss: 10698.6735\n",
                        "Step 540000, w: 2.5505, b: -147.0340, Loss: 10671.2974\n",
                        "Step 550000, w: 2.5538, b: -148.1717, Loss: 10645.4053\n",
                        "Step 560000, w: 2.5569, b: -149.2782, Loss: 10620.9168\n",
                        "Step 570000, w: 2.5600, b: -150.3543, Loss: 10597.7559\n",
                        "Step 580000, w: 2.5630, b: -151.4008, Loss: 10575.8505\n",
                        "Step 590000, w: 2.5660, b: -152.4186, Loss: 10555.1326\n",
                        "Step 600000, w: 2.5688, b: -153.4084, Loss: 10535.5378\n",
                        "Step 610000, w: 2.5716, b: -154.3709, Loss: 10517.0053\n",
                        "Step 620000, w: 2.5743, b: -155.3071, Loss: 10499.4774\n",
                        "Step 630000, w: 2.5769, b: -156.2175, Loss: 10482.8997\n",
                        "Step 640000, w: 2.5794, b: -157.1028, Loss: 10467.2207\n",
                        "Step 650000, w: 2.5819, b: -157.9639, Loss: 10452.3916\n",
                        "Step 660000, w: 2.5843, b: -158.8013, Loss: 10438.3665\n",
                        "Step 670000, w: 2.5866, b: -159.6156, Loss: 10425.1016\n",
                        "Step 680000, w: 2.5889, b: -160.4076, Loss: 10412.5558\n",
                        "Step 690000, w: 2.5911, b: -161.1778, Loss: 10400.6902\n",
                        "Step 700000, w: 2.5933, b: -161.9269, Loss: 10389.4677\n",
                        "Step 710000, w: 2.5954, b: -162.6553, Loss: 10378.8537\n",
                        "Step 720000, w: 2.5974, b: -163.3638, Loss: 10368.8150\n",
                        "Step 730000, w: 2.5994, b: -164.0528, Loss: 10359.3205\n",
                        "Step 740000, w: 2.6013, b: -164.7228, Loss: 10350.3408\n",
                        "Step 750000, w: 2.6032, b: -165.3744, Loss: 10341.8478\n",
                        "Step 760000, w: 2.6050, b: -166.0081, Loss: 10333.8152\n",
                        "Step 770000, w: 2.6068, b: -166.6244, Loss: 10326.2181\n",
                        "Step 780000, w: 2.6085, b: -167.2238, Loss: 10319.0328\n",
                        "Step 790000, w: 2.6102, b: -167.8067, Loss: 10312.2370\n",
                        "Step 800000, w: 2.6118, b: -168.3736, Loss: 10305.8097\n",
                        "Step 810000, w: 2.6134, b: -168.9249, Loss: 10299.7307\n",
                        "Step 820000, w: 2.6149, b: -169.4610, Loss: 10293.9813\n",
                        "Step 830000, w: 2.6164, b: -169.9824, Loss: 10288.5436\n",
                        "Step 840000, w: 2.6179, b: -170.4895, Loss: 10283.4006\n",
                        "Step 850000, w: 2.6193, b: -170.9826, Loss: 10278.5365\n",
                        "Step 860000, w: 2.6207, b: -171.4622, Loss: 10273.9360\n",
                        "Step 870000, w: 2.6220, b: -171.9286, Loss: 10269.5850\n",
                        "Step 880000, w: 2.6233, b: -172.3822, Loss: 10265.4698\n",
                        "Step 890000, w: 2.6246, b: -172.8233, Loss: 10261.5777\n",
                        "Step 900000, w: 2.6258, b: -173.2523, Loss: 10257.8965\n",
                        "Step 910000, w: 2.6270, b: -173.6696, Loss: 10254.4150\n",
                        "Step 920000, w: 2.6282, b: -174.0753, Loss: 10251.1221\n",
                        "Step 930000, w: 2.6293, b: -174.4699, Loss: 10248.0078\n",
                        "Step 940000, w: 2.6304, b: -174.8536, Loss: 10245.0623\n",
                        "Step 950000, w: 2.6315, b: -175.2269, Loss: 10242.2765\n",
                        "Step 960000, w: 2.6325, b: -175.5898, Loss: 10239.6417\n",
                        "Step 970000, w: 2.6336, b: -175.9428, Loss: 10237.1497\n",
                        "Step 980000, w: 2.6345, b: -176.2860, Loss: 10234.7928\n",
                        "Step 990000, w: 2.6355, b: -176.6199, Loss: 10232.5637\n",
                        "Step 1000000, w: 2.6364, b: -176.9445, Loss: 10230.4555\n",
                        "Step 1010000, w: 2.6373, b: -177.2603, Loss: 10228.4615\n",
                        "Step 1020000, w: 2.6382, b: -177.5673, Loss: 10226.5756\n",
                        "Step 1030000, w: 2.6391, b: -177.8660, Loss: 10224.7919\n",
                        "Step 1040000, w: 2.6399, b: -178.1564, Loss: 10223.1050\n",
                        "Step 1050000, w: 2.6407, b: -178.4388, Loss: 10221.5095\n",
                        "Step 1060000, w: 2.6415, b: -178.7135, Loss: 10220.0004\n",
                        "Step 1070000, w: 2.6423, b: -178.9806, Loss: 10218.5732\n",
                        "Step 1080000, w: 2.6430, b: -179.2404, Loss: 10217.2234\n",
                        "Step 1090000, w: 2.6438, b: -179.4930, Loss: 10215.9467\n",
                        "Step 1100000, w: 2.6445, b: -179.7387, Loss: 10214.7393\n",
                        "Step 1110000, w: 2.6452, b: -179.9777, Loss: 10213.5972\n",
                        "Step 1120000, w: 2.6458, b: -180.2101, Loss: 10212.5172\n",
                        "Step 1130000, w: 2.6465, b: -180.4361, Loss: 10211.4956\n",
                        "Step 1140000, w: 2.6471, b: -180.6558, Loss: 10210.5294\n",
                        "Step 1150000, w: 2.6477, b: -180.8696, Loss: 10209.6157\n",
                        "Step 1160000, w: 2.6483, b: -181.0775, Loss: 10208.7514\n",
                        "Step 1170000, w: 2.6489, b: -181.2796, Loss: 10207.9340\n",
                        "Step 1180000, w: 2.6495, b: -181.4762, Loss: 10207.1609\n",
                        "Step 1190000, w: 2.6500, b: -181.6674, Loss: 10206.4297\n",
                        "Step 1200000, w: 2.6505, b: -181.8533, Loss: 10205.7382\n",
                        "Step 1210000, w: 2.6511, b: -182.0342, Loss: 10205.0841\n",
                        "Step 1220000, w: 2.6516, b: -182.2100, Loss: 10204.4655\n",
                        "Step 1230000, w: 2.6521, b: -182.3811, Loss: 10203.8805\n",
                        "Step 1240000, w: 2.6525, b: -182.5474, Loss: 10203.3271\n",
                        "Step 1250000, w: 2.6530, b: -182.7092, Loss: 10202.8038\n",
                        "Step 1260000, w: 2.6535, b: -182.8665, Loss: 10202.3088\n",
                        "Step 1270000, w: 2.6539, b: -183.0195, Loss: 10201.8406\n",
                        "Step 1280000, w: 2.6543, b: -183.1682, Loss: 10201.3979\n",
                        "Step 1290000, w: 2.6547, b: -183.3129, Loss: 10200.9791\n",
                        "Step 1300000, w: 2.6551, b: -183.4537, Loss: 10200.5830\n",
                        "Step 1310000, w: 2.6555, b: -183.5905, Loss: 10200.2084\n",
                        "Step 1320000, w: 2.6559, b: -183.7236, Loss: 10199.8542\n",
                        "Step 1330000, w: 2.6563, b: -183.8530, Loss: 10199.5191\n",
                        "Step 1340000, w: 2.6567, b: -183.9789, Loss: 10199.2022\n",
                        "Step 1350000, w: 2.6570, b: -184.1013, Loss: 10198.9024\n",
                        "Step 1360000, w: 2.6573, b: -184.2204, Loss: 10198.6189\n",
                        "Step 1370000, w: 2.6577, b: -184.3362, Loss: 10198.3508\n",
                        "Step 1380000, w: 2.6580, b: -184.4488, Loss: 10198.0972\n",
                        "Step 1390000, w: 2.6583, b: -184.5583, Loss: 10197.8574\n",
                        "大约需要时间： 38.34799361228943\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# plot the figure\r\n",
                "plt.contourf(x, y, Z, 50, alpha=0.5, cmap=plt.get_cmap('jet'))  # 填充等高线\r\n",
                "plt.plot([-188.4], [2.67], 'x', ms=12, mew=3, color=\"orange\")\r\n",
                "plt.plot(b_history, w_history, 'o-', ms=3, lw=1.5, color='black')\r\n",
                "plt.xlim(-200, -100)\r\n",
                "plt.ylim(-5, 5)\r\n",
                "plt.xlabel(r'$b$')\r\n",
                "plt.ylabel(r'$w$')\r\n",
                "plt.title(\"线性回归\")\r\n",
                "plt.show()"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "<Figure size 432x288 with 1 Axes>"
                        ],
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQfklEQVR4nO3de6ykdX3H8feH23JR7khdLaIVN7EiC56q1Eu3iAa1XFRaNFQrQldaW1tr/4CiLmi8xCq20uzGLWqtNbUSFaQXQwPZiBWMu0XbiEJMCyXYVdSuC4uIuN/+MbOe+R3P/fLM2TPvV7LhzHNmnt9vfzn7vOeZZ+aQqkKSpD32GfYEJEnLi2GQJDUMgySpYRgkSQ3DIElqGAapL8lRSfYb9jykYTMMWtGSnJHkqQO3z03ya0km+9m/AfilCY9/a5JLFnE+FyfZuFj7k5aCz460YvUP/u8HXj2w+TnAXcDpSXZW1Z8PfO/HwMP9x74TuKW/7ZFJ9v0Z4OnAA1MMfyRwTVW9OclhwNuq6s3AQ8BPknwaOGHPeECAB6vqefP5u0qLyTMGrWS/DXwGeCjJC/rbdtE7OL8H+E6SZ6ZnFbAbeGmSw4FnAN/pb9s9cJ89fgJcVFVrJ/sDvK1/H4B9gd/sf70beKSqXlFVT6+qsaoaA84GjluaZZDmxjMGrUhJTgB+D3gJcDXwH0mOAn4VeBzwW8APgIPpBeDq/kOfBrwWOBnYBBwLFL2zjnuAc/r3+0l/nLUDj91jz7P+PWcau/t/AA6iF6bJ+GsItCx4xqCV6gzgscDNwOHAp4BTgDuAT1bVC4DzgG3As4Fv9B/3ceAkYGv/mfxfAldW1TOq6pxJxtkf+ObAM/+jmeSlpwEH0zuD2ccL3Vqu/MHUSrWxqq5K8g/ABuB/gH8EzgdenuQ5wKP6278OfJFeOO4FPgfsnOU4kz3Ln+6Z/zHA3cCJwN8meaR///1nOZ605AyDVqrqv5voXnoXkH8ZeCVwO/As4CzgQmBbVX0JIMkbAarqs0neneT9wFH9ff0O8J9V9ZoJ4xwIvCTJV/u3VwOrJtxnf8b/rT0TuL6qvkbvzIT+2I+nFydp6AyDVqpjgT+i99LRO4C/qqo/AEjy+/SuP1wIvGqyB1fVpcClSf6U3sXiv5jifl+k9w6kRpLBmz8C3pXkCfTeifTl+f2VpG4YBq1IVfW/wGOTHAKsBZ4C3Np/C+vdwJvovXR03zyHyMx36d2nqh5Isgn4PPAB4IlJPk/vra4/7d93/1nuU1pyhkErUpLTgY8B2+m9RHNjkhOBDwJ3Ao+nd9H55CT3VNVH6P172DfJ/kBV1SMT9nkAvbOH3fQO5BuTPDjFFI6id8GbJI8G/obegf+D/cefMGHfjwe+suC/uLQIDINWqi3Ar1TVtwGSnE3v4vMf968hHE/vGfxqem9Nhd7B/gDgdcDrkuz5HAJJzqV37eBieu9kuge4vKpun2zwJGcxfvBfBXwbuLQfhcl8n97LW9LQxf+Dm0ZFkgOq6uHB28ChVfW9IU5LWnYMgySp4QfcJEkNwyBJaizbi89HH310HX/88cOehiTtVbZt2/a9qjpmIftYtmE4/vjj2bp167CnIUl7lSR3L3QfvpQkSWos2zOGb98Pl9807FlI0vJ3+WmLuz/PGCRJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1DIMkqWEYJEkNwyBJahgGSVLDMEiSGoZBktQwDJKkhmGQJDU6D0OSY5Pc1vW4kqTZGcYZw/uAg4YwriRpFjoNQ5LTgF3A9i7HlSTNXmdhSHIA8Fbgkmnusz7J1iRbH9xxX1dTkyQN6PKM4RJgY1XtmOoOVbW5qsaqauzgw4/pbmaSpJ/pMgynA29IsgVYm+TqDseWJM3Sfl0NVFXP3/N1ki1VdVFXY0uSZm8on2OoqnXDGFeSNDM/4CZJahgGSVLDMEiSGoZBktQwDJKkhmGQJDUMgySpYRgkSQ3DIElqGAZJUsMwSJIahkGS1DAMkqSGYZAkNQyDJKlhGCRJDcMgSWoYhhmseeg6UrunvU9qN2seuq6jGUnS0jIM01h3/+W8asc5nLXzoinjkNrNWTsv4lU7zmHd/Zd3O0FJWgKGYQprHrqOdbuuAODkH3100jjsicLJP/ooAOt2XeGZg6S9nmGYwp2rzuS2gy742e2JcZgYBYDbDrqAO1ed2flcJWkx7TfsCSxXlX343KFXA/zs4L/nv9cfupkzd67/uSh87tCrqdhaSXs3wzCNqeIwGASYOQpXvCDznsOGG2vej5Wk+TAMM5gsDoOWMgqL8fjlyNhJy5thmIXKPlx/6OZJw3D9oZt9+WiOVmLswOBp5TAMs5DazZk710/6vTN3rvfaggCDp5XDMMxgsncfDdqzfao4bLixVuwBQ6NhJf78GrvppWp5LtDqNWO1ftPWoc5hqrekLvW7klbiP0RJS6+qSLKtqsYWsh/PGKYwVRT2HPyneivrYsRhJT6bMXbS0ksC8IyF7scwTOEpP75+2jOCqeLwzVVnc8eBZ3c/4WVuJcYODJ5WJsMwhTsOPJsth2xg3a4rpnyZaGIcthyywSiMGIOnlchrDDNY89B13LnqzGlfHkrt5ik/vt4oSMvYKMWuqhb0lzUMkrQXmyx4Cw2Db76XpL3YhhuLqvE/wLaF7tMwSJIahkGS1Oj0XUlJDgM+CewL7ALOq6qHu5yDJGl6XZ8xnA9cWVUvArYDZ3Q8viRpBp2eMVTVxoGbxwDf7XJ8SdLMhnKNIcmpwBFVdeuE7euTbE2y9cEd9w1japI08joPQ5IjgauA1038XlVtrqqxqho7+PBjup6aJImOw5DkAOAa4NKqurvLsSVJs9P1GcOFwCnAZUm2JDmv4/ElSTPo+uLzJmBTl2NKkubGD7hJkhqGQZLUMAySpIZhkCQ1DIMkqWEYJEkNwyBJahgGSVLDMEiSGoZBktQwDJKkhmGQJDUMgySpYRgkSQ3DIElqGAZJUsMwSJIahkGS1DAMkqSGYZAkNQyDJKlhGCRJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1DIMkqWEYJEkNwyBJahgGSVLDMEiSGoZBktQwDJKkxoxhSHJgFxORJC0Pszlj+EqS9yd58mIMmOTDSW5J8pbF2J8kaXHNJgwnAVuADyT5pyS/kSTzGSzJy4F9q+pU4ElJTpjPfiRJS2c2YTgM+DpwBfBp4L3Af89zvHXAp/pf3wA8d/CbSdYn2Zpk64M77pvnEJKkhdhvFvf5PnAL8G/A/cBmYOc8xzsEuLf/9Q+AUwa/WVWb+/tn9ZqxmucYkqQFmM0ZwxhwJ3AicDvwwar6yDzHewA4qP/1o2Y5viSpQzMemKvq36vqAuB84MnAF5L82TzH28b4y0cnAXfNcz+SpCUy40tJSbbQe3Z/MBBgN3Au8K55jHctcHOS1cCLgWfPYx+SpCU0m2sMrwV2AD+sqgW97l9VO5OsA14IvLeqfriQ/UmSFt+MYaiquxZzwKr6P8bfmSRJWma8+CtJahgGSVLDMEiSGoZBktQwDJKkhmGQJDUMgySpYRgkSQ3DIElqGAZJUsMwSJIahkGS1DAMkqSGYZAkNQyDJKlhGCRJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1DIMkqWEYJEkNwyBJahgGSVLDMEiSGoZBktQwDJKkhmGQJDUMgySpYRgkSQ3DIElqGAZJUmO/LgZJchjwSWBfYBdwXlU93MXYkqS56eqM4Xzgyqp6EbAdOKOjcSVJc9TJGUNVbRy4eQzw3S7GlSTN3ZKEIcmHgDUDm26qqrcnORU4oqpuneJx64H1AIc95rilmJokaQZLEoaqev3EbUmOBK4CXjHN4zYDmwFWrxmrpZibJGl6nVxjSHIAcA1waVXd3cWYkqT56eri84XAKcBlSbYkOa+jcSVJc9TVxedNwKYuxpIkLYwfcJMkNQyDJKlhGCRJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1DIMkqWEYJEkNwyBJahgGSVLDMEiSGoZBktQwDJKkhmGQJDUMgySpYRgkSQ3DIElqGAZJUsMwSJIahkGS1DAMkqSGYZAkNQyDJKlhGCRJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1DIMkqWEYJEmNTsOQ5Ngkt3U5piRpbro+Y3gfcFDHY0qS5qCzMCQ5DdgFbO9qTEnS3O23FDtN8iFgzcCmm4BfB14GXDvN49YD6wEOe8xxSzE1SdIMliQMVfX6wdtJ3gZsrKodSaZ73GZgM8DqNWO1FHOTJE2vq5eSTgfekGQLsDbJ1R2NK0maoyU5Y5ioqp6/5+skW6rqoi7GlSTNXeefY6iqdV2PKUmaPT/gJklqGAZJUsMwSJIahkGS1DAMkqSGYZAkNQyDJKlhGCRJDcMgSWoYBklSwzBIkhqGQZLUMAySpIZhkCQ1DIMkqWEYJEkNwyBJaqSqhj2HSSW5H7hj2PNYJo4GvjfsSSwTrsU412KcazFuTVU9eiE76OT/+TxPd1TV2LAnsRwk2epa9LgW41yLca7FuCRbF7oPX0qSJDUMgySpsZzDsHnYE1hGXItxrsU412KcazFuwWuxbC8+S5KGYzmfMUiShsAwSJIaQw9DksOS/EuSG5J8NskB/e0fTnJLkrcM3Pfntq00SY5NcvPA7SOS/HOSrUk+NLB95NZiYPvGJGcO3B7ltTg2yW0Dt0duLeZyDFlpJvu5WIxj59DDAJwPXFlVLwK2A2ckeTmwb1WdCjwpyQmTbRvinJdEkiOAjwGHDGx+NfCJ/nu0H51kbITXgiTPA36hqq7v3x7Zteh7H3BQ/36juhazOoZ0P9ulNdlaLNaxc+hhqKqNVfWv/ZvHAN8F1gGf6m+7AXjuFNtWmp8C5wE7B7Z9H3haksOBXwTuYUTXIsn+wF8DdyU5u795HSO4FgBJTgN20TsYwoiuxRyOISvNZD8X61iEY2fnn3zuvxyyZmDTTVX19iSnAkdU1a1Jfhe4t//9HwCn0KvixG17tWnWYvBuXwReCrwR+Aa9v/uorsVrgNuB9wJ/mOQ4RnQt+i+XvBV4GXBtf/NIrsXAfWc6huzVZrkWk/0MzPnnovMwVNXrJ25LciRwFfCK/qYH6J8eA4+id2Yz2ba92mRrMYkNwMVVtTPJnwAXMLprcTKwuaq2J/k74J30zqBGcS0uATZW1Y6BA8Oo/lzM9hiyV5vlWizKsXPoi9V/5nMNcGlV3d3fvI3x052TgLum2DYKjgBOTLIv8CygGN21+BbwpP7XY8DdjO5anA68IckWYG2SqxnRtZjDMWQULMqxczn8Er0L6Z3aXJbkMmATvVPjm5OsBl4MPJveAXHitlHwbuCjwBOAW4C/pxf0UVyLDwMfSfJKYH/gXOB+RnAtqur5e75OsqWqLkpyKCO4Fsz+GDIKrmURjp3L9pPP/SvuLwS+UFXbp9o2qlyLca7FONdi3KiuxWIcO5dtGCRJwzH0awySpOXFMEiSGoZBktQwDJKkhmGQ5iDJuiQfH/Y8pKVkGKS5WQvcNtOdpL2ZYZDm5iTgcUm+nOS/kqwb8nykRWcYpLlZC9xfVc8CLgbeMdzpSIvPMEizlGQ/4CjgXf1NXwWOHtqEpCViGKTZeyrwrap6uH/7FOBrQ5yPtCSWwy/Rk/YWJwFPTLKK3i/x2wC8abhTkhafYZBm7yTgE8CX6P1++3dU1a3DnZK0+PwlepKkhtcYJEkNwyBJahgGSVLDMEiSGoZBktQwDJKkhmGQJDUMgySp8f/V5sroKsVn2wAAAABJRU5ErkJggg=="
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}