{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "import numpy as np\r\n",
                "import pandas as pd\r\n",
                "\r\n",
                "class LinearRegression:\r\n",
                "\r\n",
                "    def __init__(self):\r\n",
                "        # the weight vector\r\n",
                "        self.W = None\r\n",
                "\r\n",
                "    def train(self, X, y, method='bgd', learning_rate=1e-2, num_iters=100, verbose=False):\r\n",
                "        \"\"\"\r\n",
                "        Train linear regression using batch gradient descent or stochastic gradient descent\r\n",
                "\r\n",
                "        Parameters\r\n",
                "        ----------\r\n",
                "        X: training data, shape (num_of_samples x num_of_features), num_of_samples rows of training sample, each training sample has num_of_features-dimension features.\r\n",
                "        y: target, shape (num_of_samples, 1). \r\n",
                "        method: (string) 'bgd' for Batch Gradient Descent or 'sgd' for Stochastic Gradient Descent\r\n",
                "        learning_rate: (float) learning rate or alpha\r\n",
                "        num_iters: (integer) number of steps to iterate for optimization\r\n",
                "        verbose: (boolean) if True, print out the progress\r\n",
                "\r\n",
                "        Returns\r\n",
                "        -------\r\n",
                "        losses_history: (list) of losses at each training iteration\r\n",
                "        \"\"\"\r\n",
                "        num_of_samples, num_of_features = X.shape\r\n",
                "\r\n",
                "        if self.W is None:\r\n",
                "            # initilize weights with values\r\n",
                "            # shape (num_of_features, 1)\r\n",
                "            self.W = np.random.randn(num_of_features, 1) * 0.001\r\n",
                "        losses_history = []\r\n",
                "\r\n",
                "        for i in range(num_iters):\r\n",
                "\r\n",
                "            if method == 'sgd':\r\n",
                "                # randomly choose a sample\r\n",
                "                idx = np.random.choice(num_of_samples)\r\n",
                "                loss, grad = self.loss_and_gradient(X[idx, np.newaxis], y[idx, np.newaxis])\r\n",
                "            else:\r\n",
                "                loss, grad = self.loss_and_gradient(X, y)\r\n",
                "            losses_history.append(loss)\r\n",
                "\r\n",
                "            # Update weights using matrix computing (vectorized)\r\n",
                "            self.W -= learning_rate * grad\r\n",
                "\r\n",
                "            if verbose and i % (num_iters / 20000) == 0:\r\n",
                "                print('iteration %d / %d : loss %f' %(i, num_iters, loss))\r\n",
                "        return losses_history\r\n",
                "\r\n",
                "\r\n",
                "    def predict(self, X):\r\n",
                "        \"\"\"\r\n",
                "        Predict value of y using trained weights\r\n",
                "\r\n",
                "        Parameters\r\n",
                "        ----------\r\n",
                "        X: predict data, shape (num_of_samples x num_of_features), each row is a sample with num_of_features-dimension features.\r\n",
                "\r\n",
                "        Returns\r\n",
                "        -------\r\n",
                "        pred_ys: predicted data, shape (num_of_samples, 1)\r\n",
                "        \"\"\"\r\n",
                "        pred_ys = X.dot(self.W)\r\n",
                "        return pred_ys\r\n",
                "\r\n",
                "\r\n",
                "    def loss_and_gradient(self, X, y, vectorized=True):\r\n",
                "        \"\"\"\r\n",
                "        Compute the loss and gradients\r\n",
                "\r\n",
                "        Parameters\r\n",
                "        ----------\r\n",
                "        The same as self.train function\r\n",
                "\r\n",
                "        Returns\r\n",
                "        -------\r\n",
                "        tuple of two items (loss, gradient)\r\n",
                "        loss: (float)\r\n",
                "        gradient: (array) with respect to self.W \r\n",
                "        \"\"\"\r\n",
                "        if vectorized:\r\n",
                "            return linear_loss_grad_vectorized(self.W, X, y)\r\n",
                "        else:\r\n",
                "            return linear_loss_grad_for_loop(self.W, X, y)\r\n",
                "\r\n",
                "\r\n",
                "def linear_loss_grad_vectorized(W, X, y):\r\n",
                "    \"\"\"\r\n",
                "    Compute the loss and gradients with weights, vectorized version\r\n",
                "    \"\"\"\r\n",
                "    # vectorized implementation \r\n",
                "    num_of_samples = X.shape[0]\r\n",
                "    # (num_of_samples, num_of_features) * (num_of_features, 1)\r\n",
                "    f_mat = X.dot(W)\r\n",
                "\r\n",
                "    # (num_of_samples, 1) - (num_of_samples, 1)\r\n",
                "    diff = f_mat - y \r\n",
                "    loss = 1.0 / 2 * np.sum(diff * diff)\r\n",
                "    \r\n",
                "    # {(num_of_samples, 1).T dot (num_of_samples, num_of_features)}.T\r\n",
                "    gradient = ((diff.T).dot(X)).T\r\n",
                "\r\n",
                "    return (loss, gradient)\r\n",
                "\r\n",
                "\r\n",
                "def linear_loss_grad_for_loop(W, X, y):\r\n",
                "    \"\"\"\r\n",
                "    Compute the loss and gradients with weights, for loop version\r\n",
                "    \"\"\"\r\n",
                "    \r\n",
                "    # num_of_samples rows of training data\r\n",
                "    num_of_samples = X.shape[0]\r\n",
                "    \r\n",
                "    # num_of_samples columns of features\r\n",
                "    num_of_features = X.shape[1]\r\n",
                "    \r\n",
                "    loss = 0\r\n",
                "    \r\n",
                "    # shape (num_of_features, 1) same with W\r\n",
                "    gradient = np.zeros_like(W) \r\n",
                "    \r\n",
                "    for i in range(num_of_samples):\r\n",
                "        X_i = X[i, :] # i-th sample from training data\r\n",
                "        f = 0\r\n",
                "        for j in range(num_of_features):\r\n",
                "            f += X_i[j] * W[j, 0]\r\n",
                "        diff = f - y[i, 0]\r\n",
                "        loss += np.power(diff, 2)\r\n",
                "        for j in range(num_of_features):\r\n",
                "            gradient[j, 0] += diff * X_i[j]\r\n",
                "            \r\n",
                "    loss = 1.0 / 2 * loss\r\n",
                "\r\n",
                "    return (loss, gradient)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "x_d = pd.DataFrame( np.asarray( [338., 333., 328., 207., 226., 25., 179., 60., 208., 606.]   ) )\r\n",
                "y_d = np.asarray( [640., 633., 619., 393., 428., 27., 193., 66., 226., 1591.]  )\r\n",
                "\r\n",
                "lr = LinearRegression()\r\n",
                "lr.train(x_d, y_d, num_iters = 300000, verbose=True)"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "ValueError",
                    "evalue": "Unable to coerce to Series, length must be 1: given 10",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-15-cd835823a0da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[1;32m<ipython-input-13-18a69f0238d2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, method, learning_rate, num_iters, verbose)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_and_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_and_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mlosses_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-13-18a69f0238d2>\u001b[0m in \u001b[0;36mloss_and_gradient\u001b[1;34m(self, X, y, vectorized)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \"\"\"\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvectorized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlinear_loss_grad_vectorized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlinear_loss_grad_for_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m<ipython-input-13-18a69f0238d2>\u001b[0m in \u001b[0;36mlinear_loss_grad_vectorized\u001b[1;34m(W, X, y)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;31m# (num_of_samples, 1) - (num_of_samples, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_mat\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# TODO: why are we passing flex=True instead of flex=not special?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[1;31m#  15 tests fail if we pass flex=not special instead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36m_align_method_FRAME\u001b[1;34m(left, right, axis, flex, level)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 474\u001b[1;33m             \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mto_series\u001b[1;34m(right)\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    466\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgiven_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m                 )\n",
                        "\u001b[1;31mValueError\u001b[0m: Unable to coerce to Series, length must be 1: given 10"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}